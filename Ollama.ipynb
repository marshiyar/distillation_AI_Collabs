{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOLWHv8pdoe9g35OhjT3T5a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marshiyar/distillation_AI_Collabs/blob/main/Ollama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Ollama\n",
        "! curl -fsSL https://ollama.com/install.sh | sh\n",
        "! npm install lshw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jG5LVY6WO3dg",
        "outputId": "58b00ea3-83bc-49ea-d496-24f26cd562d7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K\n",
            "up to date, audited 2 packages in 580ms\n",
            "\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K\n",
            "found \u001b[32m\u001b[1m0\u001b[22m\u001b[39m vulnerabilities\n",
            "\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Bj-B7aw7O_jK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Start the Ollama server in the background\n",
        "# - 'nohup' prevents the process from being killed when the terminal closes\n",
        "# - '&' sends the process to the background, allowing the cell to finish\n",
        "! nohup OLLAMA_NUM_PARALLEL=8 ollama serve > ollama.log 2>&1 &\n",
        "\n",
        "# Wait a few seconds for the server to fully start\n",
        "import time\n",
        "print(\"Waiting 10 seconds for Ollama server to start...\")\n",
        "time.sleep(10)\n",
        "print(\"Ollama server should be running.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpD0IKY2JWr6",
        "outputId": "bf738f69-9ff5-41fa-a600-5e1a6fd64eb0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting 10 seconds for Ollama server to start...\n",
            "Ollama server should be running.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ollama pull gpt-oss:20b"
      ],
      "metadata": {
        "id": "YqmP4O9LL789",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ed435e3-ae26-4695-a1b6-1a3e8367b2c0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/ai/mold_stable.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ls4-GMOXLsEG",
        "outputId": "992d653e-6f35-4f16-d590-bab423a1e38e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Scanning for completed prompts...\n",
            "üß† Teacher: gpt-oss:20b\n",
            "üìä Total Prompts: 128\n",
            "‚è≠Ô∏è  Skipping: 84 (Resuming)\n",
            "üöÄ Remaining: 44\n",
            "----------------------------------------\n",
            "[85/128] Asking: Refactor this SQL code to hand...\n",
            "‚úÖ Saved. Time: 12.25s\n",
            "[86/128] Asking: Write a unit test for a TypeSc...\n",
            "‚úÖ Saved. Time: 13.26s\n",
            "[87/128] Asking: Refactor this Rust code to han...\n",
            "‚úÖ Saved. Time: 12.31s\n",
            "[88/128] Asking: Write a C++ function to implem...\n",
            "‚úÖ Saved. Time: 21.61s\n",
            "[89/128] Asking: Write a unit test for a Bash r...\n",
            "‚úÖ Saved. Time: 15.78s\n",
            "[90/128] Asking: Explain how to build a databas...\n",
            "‚úÖ Saved. Time: 28.45s\n",
            "[91/128] Asking: Write a unit test for a Bash n...\n",
            "‚úÖ Saved. Time: 25.15s\n",
            "[92/128] Asking: Explain how to build a binary ...\n"
          ]
        }
      ]
    }
  ]
}